# ApproximateComputations.jl

[![Build Status](https://travis-ci.org/NTimmons/ApproximateComputations.jl.svg?branch=master)](https://travis-ci.org/NTimmons/ApproximateComputations.jl)[![codecov](https://codecov.io/gh/NTimmons/ApproximateComputations.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/NTimmons/ApproximateComputations.jl)

ApproximateComputations.jl is a library for the automatic applicaiton approximate computation software techniques to existing code. In this context, software approximation is where we perform some transformation to existing code to reduce the accuracy for gain in performance.

This is usually applied through function replacement. The standard workflow is to determine the maximum or average acceptable error for a given code block and then reducing the accuracy of the function so that as little work as possible is spent on gaining a more acurate result that the acceptable level.


# Tutorial
An example of the usage of the small set of functions currently in this library is avialable below.
In this small example we show how to generate a replacement for the function 'sin' that has an average error when compared to the high-precision implementation of no more than 1x10^-8.


1) Generate new functions which approximate 'sin' in the range 0.001 to pi/2 
```
using ApproximateComputations
newFunctionsAndInformation = GenerateAllApproximationFunctions(sin, 0.001, 1.57)
```

2) Filter the generated functions to select the fastest executing function within our error constraint:
```
a = GetFastestAcceptable(newFunctionsAndInformation, meanErrorLimit=0.00000001)
println(GetFunctionName(a))  # -> returns sin_PolyLet9_Float64bit
```

3) Store the function for use:
```
approxsin = a.generatedFunction
```

4) Compare to accurate function:
```
approxResult = approxsin(1.0) # -> Approximation{Float64}(0.8414709848311571)
realResult = sin(1.0)   # -> {Float64} (0.8414709848078965)
```

#### Using the Approximation type:
Approximated functions return a type wrapped in Approximation.
This is to prevent unintentional mixing of approximate space data with high precision data.

5) To convert from an approximation to the original type we use Get(x)
```
x = Get(approxResult) # -> {Float64} (0.8414709848311571)
```
#### Extra function information:
To be able to produce and select an appropriate approximation the package also contains graphing functions to show the behaviours of the different functions generated by 'GenerateAllApproximationFunctions' as well as filtering functions select only the functions which match constraints on error, runtime or internal type precision.

## AST Tutorial
We have implemented a simple AST for altering functions and breaking them into chunks for analysis. The use is relatively simple
```
using ApproximateComputations
using ImportAll
@importall Base
```
First we need to include this package and importall of Base. We need to import all the functions to allow us to override them. We need to override them so we can make them work with our wrapper types and in some cases, inject diagnostics.

We can define a normal function, in this case "little", and then submit the function to `UpdateEnvironmentForFunction`. This will identify all the functions that are called from this function and override them for our wrapper types.
```
little(x)  = (x * 2) + 5
UpdateEnvironmentForFunction(little)
```
We can then produce our AST from this object by passing in our inputs to the functions in the `Variable` type like this:
```
littletree = little(Variable(123))
printtree(littletree)

==> |+(id:3)
    |  |*(id:2)
    |  |  |123(id:1)
    |  |  |2
    |  |5
```
This shows our valid AST, and the ID for each operation.

This AST can be executed as a tree using the `EmulateTree` function
```
EmulateTree(littletree)
==> 251
```
or you can call the function directly:
```
little(123)
==> 251
```
If we want to modify the tree we can do so by supplying a replacement sub-tree and the id of the part of the tree that is to be replaced.

In this case we are changing the function from  `(123 * 2) + 5` to `(0 * 2) + 5`
```
ReplaceSubTree(littletree, Variable(0), 1)
printtree(littletree)
==> |+(id:3)
    |  |*(id:2)
    |  |  |0(id:4)
    |  |  |2
    |  |5
```
This changes the input tree and can then be ran to show that the result is now what we expect:

```
EmulateTree(littletree)
==> 5
```
Nice and simple, but allows for powerful automated transformations!


## Handling Variables with our AST

In the above example we instantiated an instance of the function AST with a constant variable. If we wanted to represent the general case of the function we need to instantiate it with a generic 'Symbol'
```
standardfunc(x)  = (x * 2) + 5
UpdateEnvironmentForFunction(standardfunc)

standardfunctree = standardfunc(Variable(:x))
```
This is now a AST which can be evaluated with any value or type for the symbol `x`.

To be able to do that we have provided two interfaces
    - `local scope variables` - These are passed in the evaluation call and set a value for different symbols for only that call.
    - `global scope variables` - These are set outside of the call to evaulate the tree and are available to any evaluation calls made afterwards.
    
There are some rules to using variables. If the variable symbol cannot be resolved an error will be returned and the symbol will be replaced with the value `0` for that call. If there are conflicting symbols in the global and local scope, the local scope will be used, this allows for convenient replacement.

Example of evaluating the above tree with a local scope instantiation of `:x` :
```
EmulateTree(standardfunctree, Dict(:x=>0) )
```
This is the equivilent of calling `standardfunc(0)`

Example of evaluating the above tree with a global scope instantiation of `:x` :
```
SetSymbolValue(:x, 0)
println( EmulateTree(standardfunctree ) )
```
This is also the equivilent of calling `standardfunc(0)`

## Loop Perforation

Loop perforation is an approximation approach where loops are manipulated to reduce the iteration count in some way to improve the performance of an application without impacting the final result beyond a fixed constraint.

This part of our library allows for the targeted replacement of loops parameters to achieve this in a generic way to arbitrary code.

We perform this optimisation on Julia AST representations. As these cannot in the current release of Julia be directly extracted from a compiled function we require that the source is directly provided, like so:
```
expr =	quote
				function newfunc()
					aa = 0
					for i in 1:10
						aa = aa + 1
					end
					aa
				end
			end
```

This example is of a simple loop which will increment a variable based on a ranged loop. This example is being used as it is trivial to test if the loop replacement has been a success.

With our function defined we pass it into the loop perforatation function:
```
LoopPerforation(expr, UnitRange, ClipFrontAndBack)
```
The function takes three inputs. The source code that is to be manipulated, the type of loop to target (in this case a loop which uses a UnitRange, others are supported and can be trivially extended to match any pattern), and a function which will output the replacement parameters. The result of calling this function is that the input source code is changed in place.

For our demo here we are passing in the source code, `UnitRange` as we are looking to replace the `1:10` loop parameter which is a `UnitRange` and the included function `ClipFrontAndBack` which takes the UnitRange and increments the minimum value and decrements the maximum value, resulting in the range `2:8`.

Once this has been called and has succeeded we can extract the function for use by evaluating the Julia Expressions:
```
eval(expr)
```
and then we can test to see if the output is what we expect
```	
@test newfunc() == 8
```
## Approximate Memoisation

Memoisation is a common technique to store the return the values of an expensive function so that multiple calls do not result in multiple expensive runs of the function.

Memoisation can be ineffecient for numerical applications with slight variations of input where each input will result a different value being saved even when they are nearly identical. In an application which is error resilient this is wasted memory as a single value could be stored for all inputs which are close or result in a similar answer.

As most memoisation is based on a hashtable we are able to construct an approximate memoisation by allowing for the custom of custom hash functions which result in clashes when any two inputs are similar enough to be within our error threshold.

With this setup, a hash function which causes a clash for any input values in 0.05 unit steps can be used to quantise the memoisation and therefore allow for arbitrary precision. A more complex hash function that allows for mapping based on the first differential of the function being memoised can give scaled memoisation boundaries based on the maximum value change across an input range - allowing for optimal storage within your acceptable error range.

To do the simple form of this with our tools we define our hash function and storage object:
```
	sinhash(fn, val) = hash(val)
	memoDict = Dict()
```
Then we call the custom hash memoisation function with our targetfunction, storage object and hash function followed by our inputs to the function.
```
	ApproximateHashingMemoise(sin, memoDict, sinhash, 0.4)
	ApproximateHashingMemoise(sin, memoDict, sinhash, 0.1)
```
The returned value of the function will either be the actual value of calling that function with the inputs, or if a value is found in the storage object for the hash of the function and input then the stored value will be returned. Nice and simple!

The weakness of this type of quantised memoisation is that the first value passed to it for any given quantisation range will represent the whole range. In some cases where there is a non-uniform access pattern for each quantisation range a value that is outside the commonly accessed part might be less representative than others.

To combat this problem we can also use a trending memoisation approach. In this approach every time a quantised range is accessed the result is added to a record and once the requisite number of samples has been taken the returned value when a hit is found is the average of all the results for that quantisation range.

The call to do this is very similiar with the addition of a `samplecount` input variable and we use a more complex storage object to enable the counting of samples for each element. In the demo below we use a sample count of `3`:

```
	trendingArray = []
	for i in 1:5000
	   push!(trendingArray,[0,0.0,0.0]) 
	end

	trendingsinhash(fn, val) = 1+Int64.(round(val*10.0))

	@test TrendingMemoisation(sin, trendingsinhash, trendingArray, 3, 0.5)  == 0.479425538604203
	@test TrendingMemoisation(sin, trendingsinhash, trendingArray, 3, 0.55) == 0.5226872289306592
	@test TrendingMemoisation(sin, trendingsinhash, trendingArray, 3, 0.51) == 0.48817724688290753
	@test TrendingMemoisation(sin, trendingsinhash, trendingArray, 3, 0.59) == 0.5563610229127838
	@test TrendingMemoisation(sin, trendingsinhash, trendingArray, 3, 0.59) == 0.5563610229127838
    
```


